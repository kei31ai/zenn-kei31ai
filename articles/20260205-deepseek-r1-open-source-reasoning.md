---
title: "DeepSeek R1が証明したこと — 推論能力は「教えなくても」出現する"
emoji: "🧠"
type: "tech"
topics: ["DeepSeek", "LLM", "強化学習", "オープンソース", "AI"]
published: true
---

中国のAIスタートアップDeepSeekが2025年1月にリリースした[DeepSeek R1](https://github.com/deepseek-ai/DeepSeek-R1)が、AI業界に衝撃を与えています。OpenAI o1に匹敵する推論性能を持ちながら、**MITライセンスで完全オープンソース**。しかも訓練コストは推定600万ドル（GPT-4の数十分の一）。この記事では、なぜDeepSeek R1が重要なのか、技術的に何が新しいのかを解説します。

---

## DeepSeek R1とは

DeepSeek R1は、中国のAI企業DeepSeekが開発した大規模推論モデルです。「推論モデル」とは、数学の問題を解いたり、コードのバグを見つけたり、論理的な思考を必要とするタスクに特化したAIモデルのことです。

**基本スペック:**
- **パラメータ数**: 671B（うち37Bがアクティブ）
- **アーキテクチャ**: Mixture of Experts (MoE)
- **コンテキスト長**: 128Kトークン
- **ライセンス**: MIT（商用利用可、改変・蒸留OK）
- **リリース日**: 2025年1月20日

671Bという巨大なパラメータ数ですが、MoE（専門家の混合）アーキテクチャにより、推論時には37Bしか使わないので、実際の計算コストは抑えられています。

---

## R1-Zeroの発見 — SFTなしで推論能力が出現した

DeepSeek R1で最も注目すべきは、**R1-Zero**という実験モデルの存在です。

従来の推論モデル（OpenAI o1など）は、まず人間が作った「正しい推論の例」を大量に学習させる**Supervised Fine-Tuning (SFT)**を行い、その後に強化学習で仕上げるという2段階のアプローチをとっていました。

DeepSeekは違うアプローチを試しました。**SFTを完全にスキップして、いきなり強化学習だけで訓練したらどうなるか？**

結果は驚くべきものでした。

> 「推論能力は純粋な強化学習だけで出現しうることを、初めてオープンな研究として実証した」
> — [DeepSeek R1 論文](https://arxiv.org/abs/2501.12948)より

具体的には、R1-ZeroはAIME 2024（数学オリンピックレベルの問題）で、訓練開始時の15.6%から最終的に71.0%まで正答率が上昇。多数決投票を使えば86.7%に達し、OpenAI o1-0912と同等の性能を示しました。

---

## 「Aha Moment」の出現

R1-Zeroの訓練中に、研究者たちは興味深い現象を観察しました。モデルが自発的に**自己反省**や**検証**を始めたのです。

論文では「Aha Moment」と呼ばれています。人間が問題を解いているときに「あ、わかった！」となる瞬間のような、突然の気づきがモデルに出現したということです。

これらの行動は明示的にプログラムされたものではありません。強化学習の報酬シグナル（正解したかどうか）だけを頼りに、モデルが自分で「考え直す」ことを学んだのです。

研究チームは、人間が定義した推論パターン（SFTで教えるもの）がむしろモデルの探索を制限してしまう可能性があると指摘しています。制約なしの強化学習の方が、新しい推論能力の出現を促すことができる、と。

---

## OpenAI o1との性能比較

では、実際の性能はどうなのか。主要なベンチマークで比較してみます。

**数学（AIME 2024）:**
- DeepSeek R1: 79.8%
- OpenAI o1: 79.2%
- → DeepSeek R1がわずかに上回る

**数学（MATH-500）:**
- DeepSeek R1: 97.3%
- OpenAI o1-1217: 同等

**コーディング（Codeforces）:**
- DeepSeek R1: 96.3パーセンタイル
- OpenAI o1: 96.6パーセンタイル
- → OpenAI o1がわずかに上回る

**一般知識（MMLU）:**
- DeepSeek R1: 90.8%
- OpenAI o1: 91.8%
- → OpenAI o1がわずかに上回る

総合的に見ると、**ほぼ同等の性能**です。数学ではDeepSeekがやや上、コーディングと一般知識ではOpenAIがやや上。誤差の範囲と言ってもいいレベルです。

---

## コストの衝撃

性能が同等なら、次に気になるのはコストです。ここでDeepSeek R1の真価が発揮されます。

**訓練コスト:**
- DeepSeek R1: 推定600万ドル
- GPT-4: 推定1億ドル以上
- → **約95%のコスト削減**

**APIコスト（100万トークンあたり）:**
- DeepSeek R1: 入力$0.55、出力$2.19
- OpenAI o1: 入力$15、出力$60
- → **約96%安い**

同じ性能のモデルを、20分の1以下のコストで使える。これは開発者にとって大きなインパクトです。

---

## なぜこれが重要なのか

DeepSeek R1の登場は、単に「安くて強いモデルが出た」という話ではありません。AI開発の構造そのものに影響を与える可能性があります。

**1. オープンソースの復権**

OpenAIやAnthropicが推論モデルをクローズドで提供する中、DeepSeekはMITライセンスで完全公開しました。モデルの重み、コード、論文、すべてが公開されています。蒸留（大きいモデルから小さいモデルに知識を移す）も許可されているので、DeepSeekの技術をベースにした派生モデルが続々と登場しています。

**2. 研究の民主化**

「推論能力は純粋なRLで出現する」という発見は、論文として公開されています。これまで「OpenAIだけが知っている秘密の手法」と思われていたものが、実は思ったより単純だったことが明らかになりました。

研究者たちは、大規模な人間によるアノテーション（SFTのためのデータ作成）がなくても、難しい問題と信頼できる検証器、そして十分な計算リソースがあれば推論能力を引き出せることを示しました。

**3. 競争の激化**

OpenAIは最先端モデルをプレミアム価格で提供するビジネスモデルでしたが、同等性能のモデルが96%安く使えるとなると、その「堀」（競争優位性）は脅かされます。「OpenAIのmoatが失われた」という声も出ています。

---

## 蒸留モデル — 小さくても強い

DeepSeekは671Bのフルモデルだけでなく、**蒸留版**も公開しています。

- DeepSeek-R1-Distill-Qwen-1.5B
- DeepSeek-R1-Distill-Qwen-7B
- DeepSeek-R1-Distill-Llama-8B
- DeepSeek-R1-Distill-Qwen-14B
- DeepSeek-R1-Distill-Qwen-32B
- DeepSeek-R1-Distill-Llama-70B

特に32Bの蒸留モデルは、OpenAI o1-miniを多くのベンチマークで上回っています。ローカルで動かせるサイズで、最先端に近い推論性能が得られるのは大きなメリットです。

[Ollama](https://ollama.com/library/deepseek-r1)経由でローカル実行も可能です。

---

## Q&A

**Q: DeepSeek R1は日本語でも使えますか？**
A: はい、多言語対応しています。ただし、英語や中国語に比べると日本語の性能は若干落ちる可能性があります。推論タスク（数学、コーディングなど）は言語に依存しにくいので、そちらでは問題なく使えるでしょう。

**Q: OpenAI o1とどっちを使うべき？**
A: コストを重視するならDeepSeek R1、既存のOpenAIエコシステムとの統合を重視するならo1。性能はほぼ同等なので、ユースケース次第です。

---

## まとめ

- **DeepSeek R1**は、OpenAI o1に匹敵する推論性能を持つオープンソースモデル
- **R1-Zero**の実験で、SFT不要で推論能力が出現することを実証
- **MITライセンス**で商用利用・蒸留OK
- **APIコストはo1の約4%**（96%安い）
- **蒸留モデル**でローカル実行も可能

「推論能力は教えなくても出現する」という発見は、AI研究の方向性に影響を与える可能性があります。そして、その研究がオープンに公開されていることが、AI開発の民主化を加速させています。

---

**参考リンク:**
- [DeepSeek R1 GitHub](https://github.com/deepseek-ai/DeepSeek-R1)
- [DeepSeek R1 論文 (arXiv)](https://arxiv.org/abs/2501.12948)
- [DeepSeek API ドキュメント](https://api-docs.deepseek.com/news/news250120)
- [Hugging Face - DeepSeek R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)
- [Nature - DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning](https://www.nature.com/articles/s41586-025-09422-z)
